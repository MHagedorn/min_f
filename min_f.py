#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Jan  3 09:05:52 2024

@author: hagedorn
"""

import warnings
import numpy as np

''' Translation of the Matlab code from
    http://www.opt.uni-duesseldorf.de/en/forschung-fs.html '''


EPS = 2.220446049250313e-16 # tolerance which is predefined in matlab


def min_f(fin, x0=None, options=None):
    '''
    AIM:
        Given a smooth function f=fin: R^n --> R  (where n >= 1)
        approximate a local optimal solution of

        minimize  f(x)  for  lb <= x <= ub

    where f is not necessarily defined for x \not\in [lb,ub].
    f may depend on additional input parameters defined in options.par_f.
    Missing vectors lb (or ub) are interpreted as -inf (or inf).

    The approximate solution generally is accurate to about 6 digits, but
    in some cases, the final error is considerably larger; it may also
    happen that the output is far away from any local optimal solution.

    Florian Jarre, last change: Feb. 9, 2018
    Thanks to Felix Lieder and Markus Lazar for numerous corrections

    Test Version with errors -
    you are free to use it; in return please report bugs to: jarre@hhu.de

    calling routine, for example:

        x             = min_f(@sin) # no bounds, just one variable

    For more than one variable, a starting point must be provided, e.g.

      [x,y,g,H,out] = min_f(@f_rosen, [-1;1;1]) # no bounds, 3 variables

    or, with additional input parameters,

      [x,y]         = min_f(@f_rosen, zeros(n,1), options)

    Mandatory input:
        A function handle fin,
        and, for n > 1, also a starting point x0 in R^n is mandatory.
        x0 is used to determine n and does not have to satisfy the bounds,
        but if it does, the returned value shall be at least as good as f(x0).

      (When x0 is not provided it is assumed that the input for f is a
       single (``scalar'') number.)

    Optional input:
        options.lb    -- a  lower bound on the variable x (Default -infty)
                        the dimension must be (n,1),
        options.ub    -- an upper bound on the variable x (Default +infty)
                        the dimension must be (n,1),
        options.par_f    If fin depends on additional parameters (not subject
                        to optimization), then the field options.par_f is a
                        struct containing the input parameters.
                        For example: function y = f(x,A)
                                          y = 0.5*x'*A*x;
                        to be called e.g. as: x = min_f(@f,ones(n,1),options)
                        where e.g. options.par_f = diag(1:n);
                        Default:   options.par_f is not provided

        options.maxit -- bound on the number of iterations; each iteration
                        with about 2*n+15 function evaluations (Default 100*n)
        options.tol   -- Only relevant for n = 1:
                        An approximate tolerance for the minimizer x:
                        In the nonsmooth case there is a local minimizer
                        xm of f satisfying |xm - x| <= tol * int_length where
                        int_length is the length of a sub interval of [lb,ub]
                        generated by the algorithm. (Default tol = 1e-8)
                        In the smooth case the above stopping criterion is
                        based on an estimate - and is not guaranteed!
                        WARNING:
                        Direct search for smooth minimization usually reaches
                        at most half the digits of full machine precision
                        (For n > 1 the stopping criterion is too unreliable
                         to allow any specification; use maxit instead to limit
                         the computational effort.)
        options.update - update gradient by central differences and Hessian by:
                        1:  PSB
                        2:  damped BFGS (default)
        options.pl     - print level (e.g. for additional output if the
                        desired accuracy is not reached), default 0
        options.mss    - Maximum Number of Slow Steps before the algoritm stops
                        Default 10


    Output:
        x:         an approximate local minimizer,
        fx:        the associated function value,
        g:         the final gradient approximation (unreliable)
                   When n=1 then g = NaN unless the spline interpolation used
                   for minimizing f is deemed reliable; then g approx. f'(x),
        H:         the final Hessian approximation (very unreliable)
                   When n=1 then H = NaN unless the spline interpolation used
                   for minimizing f is deemed reliable; then H approx. f''(x),
        out.acc:   a very unreliable (!!!) estimate for the accuracy of the
                   approximate local minimizer
        out.fval:  number of function evaluations needed,
        out.iter:  number of iterations needed, (relevant for n>1)
        out.fline: average number of function eval's in line search. (for n>1)


    Algorithm:
        For n=1 use golden mean search and acceleration by spline interpolation
        For n>1:
        Trust region algorithm with finite central difference approximation of
        the gradient using a Quasi Newton update for the Hessian.
    '''

    if x0 is None:
        x0 = 0  # no starting point, assume n = 1.

    try:
        n = len(x0)
    except:
        x0 = np.array([x0])
        n  = len(x0)

    if n == 1:
        if options is None:
            options = {'xact': x0}
        else:
            options['xact'] = x0

        try:
            options['par_f']
            def fin2(x, opt):
                return fin(x, opt), None
        except:
            def fin2(x):
                return fin(x), None

        x, fx, g, H, out = mwd11(fin2, options)
        out['fline'] = out['iter']
        out['fval']  = out['iter']  # for n = 1 this is the only
                                    # meaningful criterion
    else:
        if options is None:
            x, fx, g, H, out = mwd(fin, x0)
        else:
            x, fx, g, H, out = mwd(fin, x0, options)

    return x, fx, g, H, out




def not_NaN(x):
    ''' For minimization replace NaN with Inf '''

    if np.isnan(x):
        y = float('inf')
    else:
        y = x

    return y




def mwd(fin, x0=None, options=None):
    '''
    smooth minimzation with simple bounds without using derivatives

    calling routine, for example:
        [x,y,g,H,out] = mwd(@f_rosen, [-1;1;1])

    given a smooth function f=fin: R^n --> R with n >= 2

    minimize  f(x)  for  lb <= x <= ub

    f is not necessarily defined for x \not\in [lb,ub]

    last change: August 2016
    Test Version with errors -- No guarantee of any kind is given!

    Mandatory input:
        A function handle fin ,
        A starting point x0,

    Optional input:
        options.lb    -- a  lower bound on the variable x (Default -infty)
                        the dimension must be (n,1)
        options.ub    -- an upper bound on the variable x (Default +infty)
                        the dimension must be (n,1)
        options.maxit -- bound on the number of iterations; each iteration
                        with about 2*n+15 function evaluations (Default 100*n)
        options.update - update gradient central differences and Hessian by:
                        default 1:  PSB
                                2:  damped BFGS
        options.err   -- if the absolute error for a typical evaluation of
                        fin is known, this parameter can be set here, else it
                        is estimated (and used for the finite differences).
        options.pl     - print level (e.g. for additional output if the
                        desired accuracy is not reached), default 0
        options.mss    - Maximum Number of Slow Steps before the algoritm stops
                        Default 10

    Output:
        x:         an approximate local minimizer
        fx:        the associated function value
        g:         the final gradient approximation
        H:         the final Hessian approximation
        out.acc:   a very unreliable (!!!) estimate for the accuracy of the
                   approximate local minimizer
        out.iter:  number of iterations needed
        out.fval:  number of function evaluations needed
        out.fline: average number of function evaluations in line search

    subroutines used:
        updategh    - Update gradient and Hessian
        curvilp     - Evaluate f at a projected point along a certain curve
        mwd11       - Line search routine
    '''

# RandStream.setDefaultStream(RandStream('mt19937ar','seed',100)); #Matlab
# RandStream.setGlobalStream(RandStream('mt19937ar','seed',100)); #Matlab
# v = 1:625; rand ("state", v); #octave

    # COMPLETE INPUT ARGUMENTS
    if x0 is None:
        raise AssertionError('starting point must be supplied')
    n = len(x0)

    if options is None:
        options = {'lb': -float('inf')*np.ones(n).reshape((n, 1)),
                   'ub':  float('inf')*np.ones(n).reshape((n, 1))}

    try:
        options['lb']
    except:
        options['lb'] = -float('inf')*np.ones(n).reshape((n, 1))

    try:
        options['ub']
    except:
        options['ub'] = float('inf')*np.ones(n).reshape((n, 1))

    try:
        options['maxit']
    except:
        options['maxit'] = 300*n

    try:
        options['update']
    except:
        options['update'] = 2
    options['update'] = round(max(1, min(options['update'], 2)))
    # eliminate update-options 3-6 of earlier versions of min_f

    try:
        options['pl']
    except:
        options['pl'] = 0

    try:
        options['mss']
    except:
        options['mss'] = 10
    mss = options['mss']

    try:
        options['par_f']

        def f(x):
            return fin(x, options['par_f'])
    except:
        def f(x):
            return fin(x)

    lb   = options['lb']
    ub   = options['ub']
    o_up = options['update']

    if options['maxit'] < 2:
        print('enforce at least two iterations')
        options['maxit'] = 2
    maxit = options['maxit']

    if (ub-lb).min() < 0:
        raise AssertionError('bounds are not consistent')

    if np.shape(x0)[1] > np.shape(x0)[0]:
        x0 = x0.T  # make x0 a column vector



    # PREPARE FOR MINIMIZATION

    converged = 0  # stopping criterion for main loop (avoid the word `stop')
    iter_k    = 0  # (avoid the word 'iter')
    dt  = (1 + np.linalg.norm(x0))*EPS**(0.65)
    dtt = 2*dt

    xact = np.maximum(np.minimum(x0, ub-dtt), lb+dtt)
    if (xact-lb).min() < 0 or (xact-ub).max() > 0:
        print('lb and ub are close together, ' +
              'consider replacing with fixed var')

    xact = np.maximum(np.minimum(xact, ub), lb)
    # reproject in case that dtt is large and the above is no longer feasible
    fact = f(xact)
    iter_k += 1

    if np.isinf(not_NaN(fact)):  # i.e. if NaN, Inf or -Inf
        print('function to be minimized in min_f ' +
              'not defined at starting point')
        converged = 1

    # NOTE: When converged == 1 the gradient/Hessian Evaluation below should
    #       be omitted (correction of the code postponed since this should not
    #       happen in the first place.)

    # estimate gradient and the error of the function evaluation of f near x0
    n = len(xact)
    U = np.eye(n)       # to have a deterministic algorithm
    fval = np.zeros((n, 2))

    err = EPS
    g = np.zeros((n, 1))

    for i in range(n):
        fval[i, 0] = f(xact.flatten() + dt*U[:, i])
        fval[i, 1] = f(xact.flatten() + dtt*U[:, i])
        err = max(err, abs(fval[i, 0] - 0.5*(fact + fval[i, 1])))
        g[i] = (fval[i, 1] - fact)/dtt

    rescalef = 0
    largeg = 1
    if np.linalg.norm(g) > largeg:  # consider the problem as badly scaled
        rescalef = 1
        scaleffactor = largeg/np.linalg.norm(g)
        err = err*scaleffactor

        try:
            options['par_f']

            def f(x):
                return scaleffactor*fin(x, options['par_f'])
        except:
            def f(x):
                return scaleffactor*fin(x)

    else:
        scaleffactor = 1

    iter_k += 2*n
    try:
        options['err']
        if options['err'] < 0.1*err:
            print('either the function has large second derivative or ' +
                  'options.err was too optimistic; err is being increased')
            options['err'] = err

        if options['err'] > 10*err:
            print('options.err was more pessimistic than the estimate ' +
                  'generated by the algorithm; the pessimistic estimate ' +
                  'is used')

        err = options['err']

    except:
        options['err'] = err

    err = min(err, 1e-6)*(1 + np.linalg.norm(x0))
    # ad hoc choice - some smoothness needed
    dt = 0.1*err**(1/3)
    ub2 = ub - 1.05*dt
    lb2 = lb + 1.05*dt  # search within the smaller set to
    # allow for central differences

    if (ub2-lb2).min() < 0:
        ub2 = ub
        lb2 = lb  # hope that function values are defined outside

    xact = np.maximum(np.minimum(x0, ub2), lb2)
    fact = f(xact)

    # redo the gradient and do the Hessian
    if o_up == 1:
        H = np.zeros((n, n))

    fval = np.zeros((n, 2))

    for i in range(n):
        fval[i, 0] = f(xact.flatten() - dt*U[:, i])
        fval[i, 1] = f(xact.flatten() + dt*U[:, i])

    iter_k += 2*n
    g = (U @ (fval[:, 1]-fval[:, 0])/(2*dt)).reshape((n, 1))

    if o_up > 1:
        dbeta = (np.sum(fval, 1).reshape((n, 1)) -
                 2*fact*np.ones((n, 1)))/(dt**2)
        dbeta = np.maximum(dbeta, 1.0e-4*np.linalg.norm(dbeta)/np.sqrt(n))
        H = np.diag(dbeta.flatten())  # here, U = I (identity)

    if not_NaN(np.linalg.norm(g)) == float('inf'):
        print(' Finite difference stencil at initial point in min_f failed ')
        converged = 1

    optL = {}
    optL['lb'] = 0         # lower bound for curvi-linear search
    optL['ub'] = 1         # upper bound for curvi-linear search
    optL['err'] = err
    optL['tol'] = 1e-10    # Todo: adaptive tolerance depending on norm of
                           #       the Newton step (if defined)


    outerit   = 1
    slowsteps = 0

    while converged == 0 and outerit < maxit:  # MAIN LOOP

        outerit += 1
        ng = -np.copy(g).astype(float)

        # PROJECT THE HESSIAN
        iact = ((xact-lb2 < 1.1*dt)*(ng < 0))\
            + ((ub2-xact < 1.1*dt)*(ng > 0))
        # active indices
        inact = (np.logical_not(iact)).flatten()  # inactive indices
        if max(inact):
            # make sure, that H remains the same
            Hact = np.copy(H).astype(float)
            Hact = (Hact[inact, :])[:, inact]
        else:  # all variables are at lower or upper bounds
            Hact = np.copy(H).astype(float)  # Hact as if all
            # variables were inactive
            converged = 1
            if options['pl'] > 0:
                print('stop since all bounds are active')

        if not_NaN(np.linalg.norm(Hact, 'fro')) == float('inf'):
            Hact = np.eye(np.shape(Hact)[0])



        # LINE SEARCH
        if converged == 0:
            D, V = np.linalg.eigh(Hact)
            D = np.diag(D)  # unitary V and diagonal D so that V'*Hact*V = D
            pars = preppars(D, V, ng, inact)  # prepare parameters
            # for search step
            t0 = 0.99*(pars['opd'] + pars['ev_min']*pars['tiny'])\
                / (pars['opd'] - pars['ev_min'])
            # above corresponds to 0.99 * the plain (Quasi-) Newton step
            t0 = np.maximum(0, np.minimum(t0, 0.99))
            optL['xact'] = t0

            ocl = 1  # curvilp shall return the function value, too
            tmin, fnew, _, _, out1 =\
                mwd11(lambda t: curvilp(f, t, xact, V, lb2, ub2, pars, ocl), optL)
            if fnew >= fact:
                converged = 1
                if options['pl'] > 0:
                    print('stop since the line search ' +
                          'did not yield any improvement')

            if tmin == 0:
                converged = 1
                # this case should be redundant since then fnew >= fact
                if options['pl'] > 0:
                    print('stop since the line search ' +
                          'retruned step length zero')

            if fnew == -float('inf'):
                converged = 1
                if options['pl'] > 0:
                    print('stop since a point with function ' +
                          'value -Inf was found')

            iter_k += out1['iter']

            ocl = 0  # the function value is not needed here
            _, xnew = curvilp(f, tmin, xact, V, lb2, ub2, pars, ocl)

            if converged == 1:
                xnew = np.copy(xact).astype(float)
                fnew = fact

        else:
            xnew = np.copy(xact).astype(float)
            fnew = fact



        # UPDATE GRADIENT AND HESSIAN
        steplength = np.linalg.norm(xnew - xact)
        if steplength == 0:
            converged = 1
        else:
            if outerit == 2:
                if o_up == 2:  # for BFGS, initial H must be positive definite
                    lbd = max(0.001, abs(g.T @ (xnew - xact))
                              / np.linalg.norm(xnew - xact)**2)
                    H = lbd*np.eye(n)

            g, H = updategh(f, xnew, xact, g, H, dt, o_up)
            iter_k += 2*n + 1

        if not_NaN(np.linalg.norm(g)) == float('inf'):
            print('Finite difference stencil at ' +
                  'some iteration in min_f failed')
            converged = 1

        fold = fact
        fact = fnew
        xact = np.copy(xnew).astype(float)  # xold is not used



        # STOPPING TEST
        if converged == 0:
            converged = (steplength < 0.01*dt) and\
                (fold - fact < 0.01*dt**2/scaleffactor)
            if options['pl'] > 0 and converged == 1:
                print('stop since step length and ' +
                      'function reduction were tiny')

        if steplength < 10*dt:
            slowsteps += 1
        else:
            slowsteps = 0

        if slowsteps >= mss:
            converged = 1  # mss consecutive steps little progress
            if options['pl'] > 0:
                print('stop since ' + str(mss) +
                      ' consecutive steps little progress\n')
                print('dt is : ' + str(dt) + ' \n')
                print('last step length is : ' + str(steplength) + ' \n')

    # end of while loop



    # DO A FINAL STEP WITH ORIGINAL BOUNDS
    if fact > -float('inf') and fact < float('inf') and\
            np.linalg.norm(g) < float('inf'):
        ng = -np.copy(g).astype(float)

        iact = ((xact-lb < 0.9*dt)*(ng < 0))\
            + ((ub-xact < 0.9*dt)*(ng > 0))
        # active indices
        inact = (np.logical_not(iact)).flatten()  # inactive indices
        if max(inact):
            # make sure, that H remains the same
            Hact = np.copy(H).astype(float)
            Hact = (Hact[inact, :])[:, inact]
        else:  # all variables are at lower or upper bounds
            Hact = np.copy(H).astype(float)  # Hact as if all
            # variables were inactive

        if not_NaN(np.linalg.norm(Hact, 'fro')) == float('inf'):
            Hact = np.eye(np.shape(Hact)[0])

        D, V = np.linalg.eigh(Hact)
        D = np.diag(D)  # unitary V and diagonal D so that V'*Hact*V = D
        pars = preppars(D, V, ng, inact)  # prepare parameters for search step

        ocl = 1
        tmin, fnew, _, _, out1 =\
            mwd11(lambda t: curvilp(f, t, xact, V, lb, ub, pars, ocl), optL)

        iter_k += out1['iter']
        ocl = 0
        _, x = curvilp(f, tmin, xact, V, lb2, ub2, pars, ocl)

        if fnew >= fact:
            x = np.copy(xact).astype(float)
            fnew = fact

        fx = fnew
        g = g + H@(x - xact)  # estimate the final gradient

    else:
        x = np.copy(xact).astype(float)
        fx = fact
        inact = np.arange(n)

    if rescalef == 1:
        fx = fx/scaleffactor
        g = g/scaleffactor
        H = H/scaleffactor

    out = {}
    out['iter'] = outerit
    out['fval'] = iter_k
    out['fline'] = iter_k/outerit - 2*n-1   # 2*n+1 function evaluations
    # for gradient
    out['acc'] = not_NaN(np.linalg.norm(g[inact]))
    # assuming that inactive indices did not change in the last step

    return x, fx, g, H, out




def updategh(f, xnew, xact, g, H, dt, o_up):
    '''
    update gradient and Hessian of f

    Input:
        a function handle f
        xnew: the point at which g and H are searched for
        xact: the point at which approximations of g and H are given
        g, H: approximations of gradient and Hessian of f at xact
        dt: step length for finite difference approximation

    Output:
        gnew: approximation of gradient at xnew
        Hnew: approximation of Hessian at xnew


    Use finite differences to update gradient and PSB/BFGS for the Hessian

    Test Version with errors -- No guarantee of any kind is given!!!
    '''

    n = len(xact)
    U = np.eye(n)

    fval = np.zeros((n, 2))

    for i in range(n):
        fval[i, 0] = f(xnew.flatten() - dt*U[:, i])
        fval[i, 1] = f(xnew.flatten() + dt*U[:, i])

    gnew = U @ (fval[:, 1] - fval[:, 0]).reshape((n, 1))/(2*dt)

    dx = xnew - xact
    dxdx = np.sum(dx**2)
    if dxdx > 0.01*dt**2*n and o_up == 1:
        # PSB update only for sufficiently long steps; else
        # the finite difference error may falsify the update
        dg = gnew - g
        ddg = dg - H@dx

        dH = (np.outer(ddg, dx) + np.outer(dx, ddg))/dxdx\
            - np.outer((ddg.T@dx/dxdx**2)*dx, dx)
        dH = 0.5*(dH + dH.T)
        H = H + dH

    if dxdx > 0.01*dt**2*n and o_up == 2:
        # damped BFGS update (for sufficiently long steps)
        y = gnew - g.reshape((n, 1))
        s = dx
        ys = y.T @ s
        hs = H @ s
        shs = s.T @ hs  # H must be positive definite

        if ys < 0.01*shs:
            theta = 0.99*shs/(shs-ys)
            y = theta*y + (1-theta)*hs

        dH = np.outer(y, y/ys) - np.outer(hs, hs/shs)
        dH = 0.5*(dH + dH.T)
        H = H + dH

    Hnew = 0.5*(H + H.T)
    return gnew, Hnew




def curvilp(fin, t, x, V, lb2, ub2, pars, ocl):
    '''
    compute a point on a projected trust region curve and evaluate f
    at this point (used for a curvilinear search where 1 >= t >= 0)

    function [y,xt] = curvilp(fin,t,x,V,D,ng,lb2,ub2,inact,ocl)

    Input:
        a function handle f
        a scalar t in [0,1], a vector x
        a unitary matrix V, a diagonal matrix D
        a direction ng ``negative gradient''
        lower und upper bounds lb2 and ub2 for xt
        inact a subset of 1:n on which x is being changed
        ocl = 1 (output, CurviLp) means `evaluate f at xt'
        ocl = 0  means `return y = Inf' and save the function evaluation

    When there is no finite bound then compute
        f(x+dx)
    where
        dx = V* (D+(ev_min+(1-t)/(tiny+t))*eye(n))^(-1) *V'*ng
    and    tiny = (1.0e-10)/(1+max(abs(d)))
    and    ev_min = -min(diag(D)) + (1+abs(min(diag(D))))*10*eps.

    NOTE: This allows for very long steps even if the estimate
          H = D*D*V' of the Hessian is positive definite.
          The ``hard case'' (of More and Sorensen) is not considered here.

    In case of finite bounds keep the variables associated with active
    indices fixed and project the remaining variables onto the bounds.

    OUTPUT:
        y        -- the associated function value (if ocl == 1)
        xt       -- point along a projected curve

    last change: Feb. 2015

    Test Version with errors -- No guarantee of any kind is given!!!
    '''

    def f(x):
        return fin(x)

    al = 1  # a plane search modifying al and t simultaneously does not
            # seem to pay off

    if t < 0 or t > 1:
        xt = np.copy(x).astype(float)
        y = float('inf')
    else:
        xt = xpdx(t, al, x, V, lb2, ub2, pars)

        if ocl == 1:
            y = f(xt)
        else:
            y = float('inf')

    return y, xt




def xpdx(t, al, x, V, lb2, ub2, pars):
    ''' compute x+dx(t,alpha) = x+alpha*dx(t/alpha) with alpha in [0,1] '''

    if t < 0 or t > 1 or al < 0:
        raise AssertionError('negative step length tested')

    n = np.sum(pars['inact'])
    if n > 0:
        e = np.ones((n, 1))
        d = pars['d']
        inact = pars['inact']
        ev_min = pars['ev_min']
        tiny = pars['tiny']

        t = t/al
        dtmp = d + (ev_min + pars['opd']*(1-t)/(tiny + t))*e
        xtmp = x[inact] + al*V @ np.divide(pars['Vtng'], dtmp)

        xt = np.copy(x).astype(float)
        xt[inact] = xtmp                            # inactive indices fixed
        iactl = np.logical_not(inact) * ((x-lb2) < (ub2-x)).flatten()
        # lower active indices
        iactu = np.logical_not(inact) * ((x-lb2) >= (ub2-x)).flatten()
        # upper active indices
        xt[iactl] = lb2[iactl]
        xt[iactu] = ub2[iactu]
        xt = np.maximum(np.minimum(xt, ub2), lb2)  # project the xtmp-variables

    else:  # all indices are active (this should never happen in a line search)
        xt = np.copy(x).astype(float)
        iactl = ((x-lb2) < (ub2-x)).flatten()
        iactu = ((x-lb2) >= (ub2-x)).flatten()
        xt[iactl] = lb2[iactl]
        xt[iactu] = ub2[iactu]
        xt = np.maximum(np.minimum(xt, ub2), lb2)

    return xt




def preppars(D, V, ng, inact):
    ''' compute parameters needed for the search step '''

    n    = np.sum(inact)
    pars = {}
    pars['inact'] = inact

    if n > 0:
        d = np.diag(D).reshape((n, 1))

        pars['d']   = d
        pars['opd'] = 0.5*(1 + np.linalg.norm(d))

        ev_min = -min(d)

        pars['ev_min'] = ev_min + (1 + abs(ev_min)) * 1e-9
        pars['tiny']   = (1e-10)/(1 + max(abs(d)))
        pars['Vtng']   = (V.T @ ng[inact]).reshape((n, 1))

    else:  # all indices are active (this should never happen in a line search)
        pars['ev_min'] = EPS
        pars['tiny']   = EPS
        pars['d']      = np.zeros((0, 1))

    return pars




def mwd11(fin, options=None):
    '''
    Minimization of a continuous function f: R --> R union {Inf}

    last change: August 2016.

    Simplest calling routine

      x = mwd11(@f);

    or, with more specifications:

        [x,fx,out] = mwd11(@f,options);

    Test Version with errors -- No guarantee of any kind is given!!!

    Algorithms: Some form of bisection and spline interpolation.
            Return the lowest point that the algorithm stumbles about
            (in contrast to the official Matlab routine)


    INPUT:
        Mandatory: A function handle f,
        Optional:  The structure options as outlined below.

      options.lb   - The minimizer is restricted to the interval [lb,ub].
                      (Default of lb is -Inf)
      options.ub   - (Default of ub is  Inf)
                      If lb or ub are specified f will be evaluated only
                      within the given bounds
      options.xact - Reference point for the line search.
                      When lb <= xact <= ub the returned value shall be at
                      least as good as f(xact).

      options.tol  - An approximate tolerance for the minimizer x:
                      In the nonsmooth case there exists a local minimizer xm
                      of f satisfying |xm - x| <= tol * int_length where
                      int_length is the length of a sub interval of [lb,ub]
                      generated by the algorithm. (Default tol = 1e-8)
                      In the smooth case the above stopping criterion is
                      based on an estimate - and is not guaranteed!
                      WARNING:
                      Direct search for smooth minimization can never get more
                      than about half the digits of full machine precision



    OUTPUT:
        x        -- some approximate minimizer
        y        -- the associated function value
        g        -- NaN or an estimate of f'  at x, if available from
                  spline interpolation
        H        -- NaN or an estimate of f'' at x, if available from
                  spline interpolation
        out.iter -- specifies the number of function evaluations needed.
        out.acc  -- estimated length of final interval containing x


    Subroutines used:
          find_spline.m -- find a least squares spline through xx and ff
          eval_spline.m -- evaluate the spline at a given point
          min_spline.m  -- find the minimizer of the spline
    '''

    done = 0          # We are not done yet
    spline_int = 0    # so far no use of spline interpolation
    iterations = 0    # number of function evaluations
    out = {}          # dictionary for output



    # COMPLETE OPTIONAL INPUT AND GENERATE STARTING POINT xact in (lb,ub):

    if options is None:
        options = {}
        options['lb'] = -float('inf')
        options['ub'] = float('inf')
        options['xact'] = 0
        options['tol'] = 1e-8



    # Set x0 and x1:
    try:
        options['lb']
    except:
        options['lb'] = -float('inf')

    try:
        options['ub']
    except:
        options['ub'] = float('inf')
    x0 = options['lb']
    x1 = options['ub']

    if x1 < x0:
        raise AssertionError('lower bound larger than upper ' +
                             'bound in line search')

    try:
        options['par_f']

        def f(x):
            return not_NaN(fin(x, options['par_f'])[0])
    except:
        def f(x):
            return not_NaN(fin(x)[0])

    if x0 == x1:
        if x0 == -float('inf') or x0 == float('inf'):
            raise AssertionError('bounds in line search are inconsistent')

        options['xact'] = x0
        xact = x0
        fact = f(xact)
        f0, f1 = fact, fact
        iterations += 1
        done = 1
        warnings.warn('Trivial line search in interval of length zero')



    # Set xact:
    try:
        options['xact']
    except:
        options['xact'] = x0

    xact = options['xact']
    # Make sure xact is in the interior of [x0,x1]
    if xact <= x0 and xact >= x1:
        if x0 > -float('inf'):
            if x1 < float('inf'):
                xact = 0.5*(x0 + x1)
            else:
                t = x0 + 0.5*abs(x0) + 1

        else:  # Case x0 = -Inf and then
            if x1 < float('inf'):
                xact = x1 - 0.5*abs(x1) - 1
            else:
                xact = 0

    if done == 0:
        fact = f(xact)
        iterations += 1  # Number of function evaluations
    # Now, x0 < xact < x1  but x0 = -Inf or x1 = Inf is possible

    # Set the tolerance
    try:
        options['tol']
    except:
        options['tol'] = 1e-8

    tol = min(0.1, max(1e-10, options['tol']))  # Do not allow very high
    # precision or very low precision

    # Complete function values for x0, x1
    if done == 0:
        f0 = float('inf')
        f1 = float('inf')

    if done == 0 and x0 > -float('inf'):
        f0 = f(x0)
        iterations += 1

    if done == 0 and x1 < float('inf'):
        f1 = f(x1)
        iterations += 1




    # GENERATE FINTE BOUNDS

    dt = 1 + abs(xact)*1e-3
    # (for large xact an increment by one may be too small)
    if x1 < float('inf'):
        dt = max(dt, x1-xact)

    if x0 > -float('inf'):
        dt = max(dt, xact-x0)

    dtsave = dt


    # Make one of the bounds finite
    if x0 == -float('inf') and x1 == float('inf'):  # Here, it must be
        # ``done = 0''
        xtmp = xact + dt
        ftmp = f(xtmp)
        iterations += 1

        if ftmp < fact:
            x0 = xact
            f0 = fact
            xact = xtmp
            fact = ftmp
        else:
            x1 = xtmp
            f1 = ftmp


    # Generate a finite upper bound
    if x1 == float('inf') and f0 >= fact:  # Here, it must be ``done = 0''
        xtmp = xact + dt
        ftmp = f(xtmp)
        iterations += 1
        itcount = 0
        while itcount < 15 and ftmp < fact:  # Line search up to length 10^15
            x0   = xact
            f0   = fact
            xact = xtmp
            fact = ftmp
            dt *= 10
            xtmp = xact + dt
            ftmp = f(xtmp)
            iterations += 1
            itcount += 1

        if itcount >= 15:
            if ftmp < fact:
                xact = xtmp
                fact = ftmp
            out['iter'] = iterations
            warnings.warn('line search may be unbounded (x to Inf)')
            done = 1
        else:  # itcount < 15 means ftmp >= fact
            x1 = xtmp
            f1 = ftmp

    if x1 == float('inf') and f0 < fact:  # Here, it must be ``done = 0''
        x1   = xact
        f1   = fact
        xact = 0.5*(xact + x0)
        fact = f(xact)
        iterations += 1


    # Generate a finite lower bound
    dt = dtsave
    if x0 == -float('inf') and f1 >= fact and done == 0:
        xtmp = xact - dt
        ftmp = f(xtmp)
        iterations += 1
        itcount = 0
        while itcount < 15 and ftmp < fact:  # line search up to length 10^15
            x1   = xact
            f1   = fact
            xact = xtmp
            fact = ftmp
            dt *= 10
            xtmp = xact - dt
            ftmp = f(xtmp)
            iterations += 1
            itcount += 1

        if itcount >= 15:
            if ftmp < fact:
                xact = xtmp
                fact = ftmp
            out.iter = iterations
            warnings.warn('line search may be unbounded (x to -Inf)')
            done = 1
        else:  # itcount < 15 means ftmp >= fact
            x0 = xtmp
            f0 = ftmp

    if x0 == -float('inf') and f1 < fact and done == 0:
        x0   = xact
        f0   = fact
        xact = 0.5*(xact + x0)
        fact = f(xact)
        iterations += 1

    int_length = x1 - x0  # Length of the interval containing the minimizer
    int_length = max(int_length, 10*EPS*(abs(x0) + abs(x1))/tol)
    tol = max(tol, 10*EPS*(1 + abs(x0) + abs(x1))/int_length)


    # Eliminate Inf-values of f
    if min([f0, fact, f1]) == float('inf'):
        warnings.warn('No finite value of f found in line search')
        done = 1

    itcount = 0
    if fact == float('inf') and done == 0:
        if f1 < f0:  # look for minimizer near x1
            while fact == float('inf') and itcount < 15:
                itcount += 1
                x0 = xact
                f0 = fact
                xact = 0.1*xact + 0.9*x1
                fact = f(xact)
                iterations += 1
            if fact == float('inf'):
                warnings.warn('Only infinite objective ' +
                              'values found in line search')
                done = 1

        else:  # look for minimizer near x0
            while fact == float('inf') and itcount < 15:
                itcount += 1
                x1 = xact
                f1 = fact
                xact = 0.1*xact + 0.9*x0
                fact = f(xact)
                iterations += 1
            if fact == float('inf'):
                warnings.warn('Only infinite objective ' +
                              'values found in line search')
                done = 1
    # Now, fact is finite


    if f1 == float('inf') and done == 0:
        while f1 == float('inf') and itcount < 30:
            x1old = x1
            x1 = 0.5*(xact + x1)
            f1 = f(x1)
            iterations += 1
        if f1 == float('inf'):
            warnings.warn('Only infinite objective ' +
                          'values found in line search')
            done = 1
        else:
            if f1 < min(f0, fact):
                x0   = xact
                f0   = fact
                xact = x1
                fact = f1
                x1   = x1old
                f1   = float('inf')

    if f0 == float('inf') and done == 0:
        while f0 == float('inf') and itcount < 30:
            x0old = x0
            x0 = 0.5*(xact + x0)
            f0 = f(x0)
            iterations += 1
        if f0 == float('inf'):
            raise AssertionError('Only infinite objective values ' +
                                 'found in line search')
        else:
            if f0 < min(f1, fact):
                x1   = xact
                f1   = fact
                xact = x0
                fact = f0
                x0   = x0old
                f0   = float('inf')



    # MAKE SURE f0 AND f1 ARE LARGER THAN fact

    itcountmax = 9 + round(-np.log(tol)/np.log(10))

    # higher precision near end points
    # Number of iterations to identify a minimizer near the boundary

    if fact >= min(f0, f1) and done == 0:
        itcount = 0
        if f0 < f1:
            while fact >= f0 and itcount < itcountmax:
                x1 = xact
                f1 = fact
                itcount += 1
                xact = 0.9*x0 + 0.1*x1
                fact = f(xact)
        else:
            while fact >= f1 and itcount < itcountmax:
                x0 = xact
                f0 = fact
                itcount += 1
                xact = 0.1*x0 + 0.9*x1
                fact = f(xact)

        iterations += itcount
        if itcount >= itcountmax or x0 == xact or x1 == xact:
            done = 1
            if f0 < min(f1, fact):
                xact = x0
                fact = f0
            if f1 < min(f0, fact):
                xact = x1
                fact = f1
            out['iter'] = iterations

    # Now, either ``x1-x0 <= tol*int_length'' or ``fact < min(f0,f1)''
    if xact <= x0 or x1 <= xact or f0 < fact or f1 < fact:
        if done == 0:
            raise AssertionError(' programming error in line search 1')



    # NOW, THE ACTUAL LINE SEARCH APPROXIMATING A MINIMIZER IN [x0,x1]

    itcount = 0                      # iteration counter
    gm6 = 2/(np.sqrt(5)+1)           # Golden mean ratio, this is about 0.6
    xfval = np.vstack([np.hstack([x0, xact, x1]), np.hstack([f0, fact, f1])])
                                     # Record all values of the search
    iact = 2                         # Index of xact in xfval
    lref = 0                         # last refinement not used so far


    while done == 0:  # *** MAIN LOOP ***
        spline_int = 0
        itcount += 1

        # One golden mean search step
        if x1 - xact > xact - x0:
            xa = x1 + gm6*(xact - x1)
            fa = f(xa)
            iterations += 1
            xfval = np.hstack([xfval[:, :iact],
                               np.vstack([xa, fa]), xfval[:, iact:]])

            if fa <= fact:
                x0   = xact
                f0   = fact
                xact = xa
                fact = fa
                iact += 1
            else:
                x1 = xa
                f1 = fa
        else:
            xa = x0 + gm6*(xact - x0)
            fa = f(xa)
            iterations += 1
            xfval = np.hstack([xfval[:, :iact-1],
                               np.vstack([xa, fa]), xfval[:, iact-1:]])

            if fa <= fact:
                x1   = xact
                f1   = fact
                xact = xa
                fact = fa
            else:
                x0 = xa
                f0 = fa
                iact += 1
        # end of golden mean step
        if x1 - x0 <= tol*int_length:
            done = 1


        # Test whether to use spline interpolation
        n = len(xfval.T)
        if n >= 5 and done == 0:
            # Check whether the spline would have predicted fact correctly
            # Find points close to xact on both sides
            # (2(dx[1:n-1] + dx[:n-2])* <= iact <= end-1)
            if n == 5:
                indspl = np.hstack([np.arange(1, iact), np.arange(iact+1, 6)])
                # "Spline" with 4 interpolat. points

            else:
                # Strategy: Choose 5 points, (if possible) two larger
                # than xact, two smaller, and the last one the closest
                # to xact (among the rest).
                i_set = 0  # indspl not yet set
                if iact <= 2:
                    if iact <= 1:
                        raise AssertionError('programming error ' +
                                             'in line search 2')
                    indspl = np.array([1, 3, 4, 5, 6])
                    i_set = 1  # do not change indspl any more
                n = len(xfval.T)
                if iact >= n-1:
                    if iact >= n:
                        raise AssertionError('programming error ' +
                                             'in line search 3')
                    indspl = np.array([n-5, n-4, n-3, n-2, n])
                    i_set = 1  # do not change indspl any more
                if i_set == 0:  # now, 3 <= iact <= n-2
                    indspl0 = np.array([iact-2, iact-1, iact+1, iact+2])
                    tmp = np.hstack([-float('inf'), xfval[0, :],\
                                     float('inf')])  # Note:tmp(iact+1)=xact
                    if tmp[iact] - tmp[iact-3] < tmp[iact+3]-tmp[iact]:
                        indspl = np.hstack([iact-3, indspl0])
                    else:
                        indspl = np.hstack([indspl0, iact+3])
            # Index for spline is set

            xx = xfval[0, indspl-1]
            ff = xfval[1, indspl-1]

            ss = find_spline(xx.reshape((1, len(indspl))),
                             ff.reshape((1, len(indspl))))

            sact, i = eval_spline(xact, ss, xx.reshape((1, len(indspl))))
            ip1 = min(1+1, len(xx)-1) - 1
            if abs(sact-fact) < 0.1*(abs(ss[2, i]) + abs(ss[2, ip1])) *\
                    (xact - xx[i])*(xx[i+1] - xact):
                spline_int = 1  # Estimate of second derivative is 80# correct



        while spline_int == 1 and itcount < 100 and done == 0:
            # use minimizer of the spline function (possibly several steps)
            itcount += 1

            # Recompute the spline including xact
            indspl = np.arange(iact-2, iact+3)
            n = len(xfval.T)

            if iact <= 1 or iact >= n:
                raise AssertionError('programming error in line search 4')
            if iact == 2:
                indspl = np.arange(1, 6)
            if iact == n-1:
                indspl = np.arange(n-4, n+1)

            xx = xfval[0, indspl-1]
            ff = xfval[1, indspl-1]

            ss = find_spline(xx.reshape((1, len(indspl))),\
                             ff.reshape((1, len(indspl))))

            t, tval, i = min_spline(ss, xx.reshape((1, len(indspl))))

            # SPLINE MINIMIZER t in [xx(i),xx(i+1)]
            # x0 = xfval(1,iact-1), x1 = xfval(1,iact+1)
            if t <= x0 or t >= x1:
                spline_int = 0

            else:  # Spline interpolation may be o.k. ###

                ixf = i+indspl[0] - 1  # t in [xfval(1,ixf),xfval(1,ixf+1)]

                if t < xx[i] or t > xx[i+1] or t < xfval[0, ixf] or\
                        t > xfval[0, ixf+1]:
                    raise AssertionError('programming error in ' +
                                         'spline-line search 5')

                shift_t = 0.1*min(tol*int_length, xx[i+1] - xx[i])
                # the shift is (much) less than tol*int_length
                t = max(t,   xx[i] + shift_t)
                t = min(t, xx[i+1] - shift_t)
                tnewton = xact - 0.5*ss[1, 2] / ss[2, 2]
                                                    # Newton step for spline
                safetygap = min(abs(t-tnewton), 0.1*min(x1 - xact, xact - x0))
                # If xact is close to x0 or x1 and t is close to xact on the
                # other side, convergence may be slow ==> move a bit away from
                # xact

                if xact - x0 < 0.1*(x1 - xact) and t > xact:
                    t = t + safetygap
                if x1 - xact < 0.1*(xact - x0) and t < xact:
                    t = t - safetygap

                ft = f(t)
                iterations += 1
                epp = abs(tval - ft)  # The approximation error at t
                ip1 = min(1+1, len(xx) - 1) - 1
                if epp > 0.2*(abs(ss[2, i]) +
                              abs(ss[2, ip1]))*(t-xx[i])*(xx[i+1]-t):
                    spline_int = 0  # Prediction not so accurate

                if epp < 100*EPS*(abs(ft)+1):
                    done = 1       # Prediction is close to machine precision
                    spline_int = 1  # keep spline interpolation for derivatives

                epp = 2*epp/((t-xx[i])*(xx[i+1]-t))  # Second der. of the error
                mss = ((xx[i+1]-t)*ss[2, i]+(t-xx[i])\
                       * ss[2, ip1])/(xx[i+1]-xx[i])
                if mss > epp:
                    tmp = max(0.1*(xx[i+1]-xx[i]), abs(0.5*(xx[i+1]+xx[i])-t))
                    if tmp*epp/(mss-epp) < int_length*tol:
                        done = 1       # prediction of error in t is small
                        spline_int = 1  # keep spline interpolation for
                        # derivatives

                xfval = np.hstack([xfval[:, :ixf+1],\
                                   np.vstack([t, ft]), xfval[:, ixf+1:]])

                if t < xact:
                    if ft > fact:
                        iact += 1  # xact remains same but iact increases by 1
                else:
                    if ft < fact:
                        iact += 1  # xact changes and also iact increases by 1

                x0   = xfval[0, iact-2]
                f0   = xfval[1, iact-2]
                xact = xfval[0, iact-1]
                fact = xfval[1, iact-1]
                x1   = xfval[0, iact]
                f1   = xfval[1, iact]
                if x1-x0 <= tol*int_length:
                    done = 1
                    spline_int = 1  # keep spline interpolation for derivatives

            ### Case where spline interpolation may be o.k. ###

            dddx = xfval[0, 1:] - xfval[0, :-1]
            if min(dddx) < 10*EPS*(1+abs(x0) + abs(x1)):
                done = 1

        if x1-x0 <= tol*int_length or itcount >= 100:
            done = 1

        dddx = xfval[0, 1:] - xfval[0, :-1]
        if min(dddx) < 10*EPS*(1 + abs(x0) + abs(x1)):
            done = 1

        if done == 1:  # do a (final?) check
            check = 0
            acc = max(x1 - xact, xact - x0)
            if acc > int_length*tol:
                check = 1
            if fact >= min(f0, f1):
                check = 0
            if min(x1 - xact, xact - x0) <= 0:
                check = 0
            if check > 0:
                slope0 = (f0 - fact)/(x0 - xact)
                slope1 = (f1 - fact)/(x1 - xact)
                t0 = 0.5*(x0 + xact)
                t1 = 0.5*(x1 + xact)
                topt = t0 - slope0*(t1 - t0)/(slope1 - slope0)
                if abs(xact - topt) < int_length*tol:
                    check = 0

            if check > 0:
                if x1 - xact > xact - x0:
                    t  = xact + 0.9*int_length*tol
                    ft = f(t)
                    iterations += 1
                    xfval = np.hstack([xfval[:, :iact],\
                                       np.vstack([t, ft]), xfval[:, iact:]])

                    if ft < fact:
                        done = 0
                        lref = 1
                        iact += 1  # xact changes and also iact increases by 1
                        x0   = xact
                        f0   = fact
                        xact = t
                        fact = ft
                    else:
                        x1 = t
                        f1 = ft

                else:  # we have x1-xact <= xact-x0
                    t  = xact - 0.9*int_length*tol
                    ft = f(t)
                    iterations += 1

                    xfval = np.hstack([xfval[:, :iact-1],\
                                       np.vstack([t, ft]), xfval[:, iact-1:]])

                    iact += 1
                    if ft < fact:
                        done = 0
                        lref = 1
                        iact -= 1  # xact changes and also iact increases by 1
                        x1   = xact
                        f1   = fact
                        xact = t
                        fact = ft
                    else:
                        x0 = t
                        f0 = ft

                acc = max(x1-xact, xact-x0)
                if done == 1 and acc > int_length*tol:
                    # repeat the above (automatically this is the other side)
                    if x1 - xact > xact - x0:
                        t  = xact + 0.9*int_length*tol
                        ft = f(t)
                        iterations += 1

                        xfval = np.hstack([xfval[:, :iact],\
                                    np.vstack([t, ft]), xfval[:, iact:]])

                        if ft < fact:
                            done = 0
                            lref = 1
                            iact += 1  # xact changes, also iact decreases by 1
                            x0   = xact
                            f0   = fact
                            xact = t
                            fact = ft
                        else:
                            x1 = t
                            f1 = ft
                    else:  # we have x1-xact <= xact-x0
                        t  = xact - 0.9*int_length*tol
                        ft = f(t)
                        iterations += 1
                        xfval = np.hstack([xfval[:, :iact-1],\
                                    np.vstack([t, ft]), xfval[:, iact-1:]])

                        iact += 1
                        if ft < fact:
                            done = 0
                            lref = 1
                            iact -= 1  # xact changes, also iact decreases by 1
                            x1   = xact
                            f1   = fact
                            xact = t
                            fact = ft
                        else:
                            x0 = t
                            f0 = ft

    # *** END OF MAIN LOOP ***
    x = xact
    y = fact
    out['iter'] = iterations
    spline_int = max(spline_int, lref)

    if spline_int == 1 and y > -float('inf'):
        n = len(xfval.T)
        if xact < xfval[0, 0] or xact > xfval[0, n-1]:
            warnings.warn('final point out of range in spline evaluation')
            g = float('nan')  # No derivative information available from Spline
            H = float('nan')  # (Use finite difference instead - not done here)

        else:
            # Recompute the spline including xact
            indspl = np.arange(iact-2, iact+3)
            if iact <= 1 or iact >= n:
                raise AssertionError('programming error in line search 6')
            if iact == 2:
                indspl = np.arange(1, 6)
            if iact == n-1:
                indspl = np.arange(n-4, n+1)
            xx = xfval[0, indspl-1].reshape((1, len(indspl)))
            ff = xfval[1, indspl-1].reshape((1, len(indspl)))
            ss = find_spline(xx, ff)

            i = 0
            while xact > xx[0, i+1]:
                i = i+1

            t = xact - xx[0, i]

            g = ss[1, i] + t*(2*ss[2, i] + t*(3*ss[3, i]))

            H = 2*ss[2, i] + t*(6*ss[3, i])

    else:
        g = float('nan')  # No derivative information available from Spline
        H = float('nan')  # (Use finite difference instead - not done here)

    acc = max(x1 - xact, xact - x0)
    if spline_int == 1:
        if np.isnan(g):
            out['acc'] = acc
        else:
            out['acc'] = min(abs(g)/(EPS + abs(H)), acc)
    else:
        out['acc'] = acc

    return x, y, g, H, out






def find_spline(x, f):
    ''' Find the least squares spline interpolant through the points x(i), f(i)
        assuming that there are at least 4 interpolation points. On each
        interval [x(j),x(j+1)] (1<=j<=n-1) the spline is represented as
        s(x) = s(1,j) + s(2,j)*(x-x(j)) + s(3,j)*(x-x(j))^2 + s(4,j)*(x-x(j))^3
        '''

    # Initialize
    n, m = np.shape(x)
    if n < m:  # make x a column vector
        x = x.T
        n, m = m, n

    if m > 1 or m == 0 or n < 4:
        raise AssertionError('input x for spline interpolation ' +
                             'is inconsistent')

    nn, mm = np.shape(f)
    if nn < mm:  # make f a column vector
        f = f.T
        nn, mm = mm, nn

    if mm > 1 or m == 0:
        raise AssertionError('input f for spline interpolation ' +
                             'is inconsistent')

    if n != nn:
        raise AssertionError('input x,f for spline interpolation ' +
                             'is inconsistent')

    dx = x[1:] - x[:n-1]  # vector of increments of x

    if min(dx) <= 0:
        raise AssertionError('vector of spline base points is assumed ' +
                             'to be in increasing order')


    s = np.zeros((12, n))  # Three splines with n-1 cubic parts each.
    # The last column is a dummy to allow assigning f as the first row.
    # The first spline interpolates f,
    # the second and third interpolate the zero function.
    # rows 1 to 4  for the spline interpolating f
    # rows 5 to 8  for the spline interpolating zero; nonzero linear first term
    # rows 9 to 12 for the spline interpolating zero; nonzero quadr. first term
    t1 = [0, 4,  8]  # indices for the constant terms
    t2 = [1, 5,  9]  # indices for the linear terms
    t3 = [2, 6, 10]  # indices for the quadratic terms
    t4 = [3, 7, 11]  # indices for the cubic terms


    s[0, :] = f.flatten()  # the constant part of s(1:4,:) is given by f
    # (constant parts of the zero splines are zero)

    # Set up the three splines; first the splines on [x(1),x(2)]
    s[1, 0]  = (f[1]-f[0])/dx[0]  # first spline, linear on [x(1),x(2)]
    s[5, 0]  = 1                  # second spline, first linear term is 1
    s[7, 0]  = -1/dx[0]**2        # interoplate to zero at x(2)
    s[10, 0] = 1                  # third spline, first quadratic term is 1
    s[11, 0] = -1/dx[0]           # interoplate to zero at x(2)


    # then the interpolating splines on the remaining subintervals
    for i in range(1, n-1):
        s[t2, i] = s[t2, i-1] + dx[i-1]*(2*s[t3, i-1] + 3*dx[i-1]*s[t4, i-1])
        s[t3, i] = s[t3, i-1] + 3*dx[i-1]*s[t4, i-1]
        s[t4, i] = (s[t1, i+1]-s[t1, i] - dx[i] *
                    (s[t2, i]+dx[i]*s[t3, i]))/dx[i]**3

    s = s[:, :n-1]  # now remove the last column

    # The norm is given by || D * \Delta * s(4,:)' ||_2 where, formally,
    # D      = Diag(((dx(2:n-1)+dx(1:n-2)).^-.5); and
    # \Delta = [-eye(n-2),zeros(n-2,1)]+[zeros(n-2,1),eye(n-2)];
    # i.e. a weighted sum of squared jumps of the third derivatives


    d = (dx[1:n-1] + dx[:n-2])**(-0.5)  # weights
    #d  = 1/np.sqrt(dx[1:n-1] + dx[:n-2])
    Ds = (np.kron(np.ones((3, 1)), d.T) * (s[t4, :n-2] - s[t4, 1:n-1])).T
    # Ds contains the weighted jumps in the cubic terms of the splines


    # orthogonalize the third with respect to the second spline
    alpha = Ds[:, 2].T @ Ds[:, 1]/(Ds[:, 1].T @ Ds[:, 1])
    # updating s(10:12,:) would suffice
    s[8:12, :] = s[8:12, :] - alpha*s[4:8, :]
    Ds[:, 2]   = Ds[:, 2] - alpha*Ds[:, 1]


    # adjust first with the second spline
    alpha = Ds[:, 0].T @ Ds[:, 1]/(Ds[:, 1].T @ Ds[:, 1])
    s[:4, :] = s[:4, :] - alpha*s[4:8, :]  # updating s(2:4,:) would suffice
    Ds[:, 0] = Ds[:, 0] - alpha*Ds[:, 1]


    # adjust first with the third spline
    alpha = Ds[:, 0].T @ Ds[:, 2]/(Ds[:, 2].T @ Ds[:, 2])
    s[:4, :] = s[:4, :] - alpha*s[8:12, :]  # updating s(2:4,:) would suffice
    # Ds(:,1)   = Ds(:,1) -alpha*Ds(:,3); # not needed

    # to reduce rounding errors redo the final spline
    s = s[:4, :]

    for i in range(1, n-1):
        s[1, i] = s[1, i-1] + dx[i-1]*(2*s[2, i-1] + 3*dx[i-1]*s[3, i-1])
        s[2, i] = s[2, i-1] + 3*dx[i-1]*s[3, i-1]
        s[3, i] = (f[i+1] - f[i] - dx[i]*(s[1, i] + dx[i]*s[2, i]))/dx[i]**3

    return s






def eval_spline(xact, s, x):
    ''' Evaluate the spline given by s and the partition x at the point xact
        Also return the segment in which xact is located
        Assume x in increasing order, length(x) = n and s is 1:4 by 1:n-1 '''

    n = len(x.T)

    if xact < x[0, 0] or xact > x[0, n-1]:
        raise AssertionError('point out of range in spline evaluation')

    i = 0
    while xact > x[0, i+1]:
        i += 1

    t = xact - x[0, i]
    y = s[0, i] + t*(s[1, i] + t*(s[2, i] + t*(s[3, i])))

    return y, i






def min_spline(s, x):
    ''' Find the minimum of the spline given by s in the interval [x(1),x(end)]
        The minimizer t with value y in the interval [x(imin),x(imin+1)]
        Assume x in increasing order, length(x) = n and s is 1:4 by 1:n-1 '''

    x = x[0]
    n = len(x)

    # Test support points first
    y, imin = min(s[0, :]), np.argmin(s[0, :])

    t = x[imin]
    dx = x[n-1] - x[n-2]

    y_last = s[0, n-2] + dx*(s[1, n-2] + dx*(s[2, n-2] + dx*(s[3, n-2])))

    if y_last < y:
        y = y_last
        t = x[n-1]
        imin = n-2  # Not imin = n, so x(imin+1) exists


    # y is the smallest value so far
    # for i = 1:n-1
    for i in range(n-1):
        v = s[:, i]  # just for convenience

        if v[3] != 0:
            discr = v[2]**2 - 3*v[3]*v[1]
            if discr >= 0:
                tmp = v[2] + np.sign(v[2])*np.sqrt(discr)
                t1 = -tmp/(3*v[3])

                if t1 > 0 and t1 < x[i+1]-x[i]:
                    y1 = s[0, i] + t1*(s[1, i] + t1*(s[2, i] + t1*(s[3, i])))
                    if y1 < y:
                        y = y1
                        t = x[i] + t1
                        imin = i

                t1 = -v[1]/tmp
                if t1 > 0 and t1 < x[i+1]-x[i]:
                    y1 = s[0, i] + t1*(s[1, i] + t1*(s[2, i] + t1*(s[3, i])))
                    if y1 < y:
                        y = y1
                        t = x[i] + t1
                        imin = i

        else:
            if v[2] != 0:   # when this is instable then the minimizer is
                # outside the current interval
                t1 = -0.5*v[1]/v[2]

                if t1 > 0 and t1 < x[i+1]-x[i]:
                    y1 = s[0, i] + t1*(s[1, i] + t1*(s[2, i] + t1*(s[3, i])))

                    if y1 < y:
                        y = y1
                        t = x[i] + t1
                        imin = i

    return np.array([t]), y, imin
